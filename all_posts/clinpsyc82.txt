some of my longstanding readers probably remember that i long ago wrote about a statistical issue in the seroquel trials for bipolar depression known by the corny acronym bolder it was just a minor issue , you know , the kind that would make a drug look about more effective than a placebo depending on which type of analysis you chose to use no biggie lilly started it it just so happens that philip dawdy who has apparently been christened as dr dawdy at furious seasons recently had a letter published in the journal of clinical psychopharmacology on this issue of statistics dawdy noted that the authors' use of a statistical method known as mixed models repeated measures mmrm rather than the more conventional last observation carried forward locf resulted in a major inflation in effect size as i mentioned earlier , the choice of methods to calculate the effect size the magnitude of difference between drug and placebo had a big impact dawdy aptly noted that the authors should have reported the effect sizes calculated by both methods so that readers could note how one method made seroquel look better than did the other method to quote dawdy , the authors should also have reported the locf effect sizes so that the readers would have been aware of how the method impacted the findings i was flattered to see that my blog was cited in dawdy 's letter i heard through the grapevine that another author attempted to cite my blog in a letter to the editor , but that the journal struck the citation to my site in the final version of the published letter if some of y'all researchers who read this blog wanna cite my site , go ahead i'm not saying that seroquel was a dud , but that it did get a boost from the analysis used in the study when the authors are playing by a new rule when it comes to calculating the differences between drug and placebo , it would make sense to report the results using both the old rules and new rules in his response , michael thase of the bolder team responded that it is my understanding that mixed model repeated measurement mmrm analysis was chosen to compute effect sizes in the bolder studies because it would permit direct comparison with the results of the study of the only other treatment approved for bipolar depression , the combination of olanzapine and fluoxetine ofc thus , in plain and simple terms , we were attempting to facilitate an apples to apples comparison between quetiapine monotherapy and ofc so because lilly did it , we did it um , ok but is there some kind of law against reporting the results from both the newfangled mmrm analysis and the old fashioned locf analysis ? just wondering and if lilly started saying it was okay to market zyprexa off label for various conditions , would that mean all antipsychotics could be marketed off label for all sorts of issues ? hypothetically speaking , of course stats thase goes on to note that there is some research suggesting that mmrm does not overinflate effect sizes rather , locf underestimates them i know a bit about stats , but i'm not a statistician basically , the differences between the methods boil down to how data is handled for persons who dropped out of a study the best solution is to try to track down study dropouts and assess how they are functioning , rather than having a statistical model guess at their sense of mental well being , but this requires extra effort and time , and is sometimes not possible basically , the locf model makes some assumptions that are quirky at best , while mmrm seems to handle missing data better in many situations all that being said , in many trials where a drug beats placebo , mmrm appears to generate effect sizes that are higher than locf , which then leads us to a question geez , have we been underestimating the effects of drugs by ? um , that seems a little hard to swallow i'm not quite ready to buy into that