i mentioned in a previous post that i once interviewed for a job at a well known search company one of the five people who interviewed me asked a question that resulted in an hour long discussion explain how you would develop a frequency sorted list of the ten thousand most used words in the english language i'm not sure why anyone would ask that kind of question in the course of an interview for a technical writing job it 's more of a software design kind of question , but it led to a lively discussion , and i still think it 's one of the best technical interview questions i 've ever heard ask yourself how would you answer that question ? my initial response was to assail the assumptions underlying the problem language is a fluid thing , i argued it changes in real time vocabulary and usage patterns shift day to day to develop a list of words and their frequencies means taking a snapshot of a moving target whatever snapshot you take today is n't going to look like the snapshot you take tomorrow or even five minutes from now so the first question is where do we get our sample of words from ? is this about spoken english , or written english ? two different vocabularies with two different frequency patterns but again , each is mutable , dynamic , fluid , protean , changing minute by minute , day by day suppose we limit the problem to written english how will we obtain a representative sampling of english prose ? it should be obvious that there is no such thing there is no average corpus think about it my interviewer wanted to cut the debate short and move on to algorithms and program design , but i resisted , pointing out that problem definition is extremely important you ca n't rush into solving a problem before you understand how to pose it let 's assume , my inquisitor said , that the web is a good starting place english web pages i tormented my tormentor some more , pointing out that it 's dangerous to assume spiders will crawl pages in any desirable e g , random fashion , and anyway , some experts believe deep web content content that 's either uncrawlable or has never been crawled before constitutes the majority of online content so again , we 're not likely to obtain any kind of representative sample of english words , if there even is such a thing as a representative sample of the english language which i firmly maintain there is not by now , my interviewer was clearly growing impatient with my petulence , so he asked me to talk about designing a program that would obtain a sorted list of , most used words i dutifully regurgitated the standard crawl canonicalize parse tally sorts of things that you 'd typically do in such a program how would you organize the words in memory ? my tormentor demanded to know a big hash table , i said just hash them right into the table and bump a counter at each spot how much memory will you need ? what 've you got ? i smiled no , seriously , how much ? he said i said assuming bit hardware and software , maybe something like gigs enough memory for a billion slot array of bytes of data per slot most words will fit in that space , and a short int will suffice for a counter in each slot longer words can be hashed into a separate smaller array meanwhile you 're using bits available but you 're only using of address space , which is enough to hash words of length or less with no collisions at all the typical english word has entropy of about bits per character longer words entail some risk of hash collision , but with a good hash function that should n't be much of a problem what kind of hash function would you use ? the interviewer asked i 'd try a very simple linear congruential generator , for speed , i said , and see how it performs in terms of collisions he asked me to draw the hash function on the whiteboard i scribbled some pseudocode that looked something like hash initial value for each char in word hash magic number hash char hash bounds return hash i explained that the hash table array length should be prime , and the bounds number is less than the table length , but coprime to the table length good possible values for the magic number might be , , or or other small primes you can test various values until you find one that works well what will you do in the event of hash collisions ? the professor asked how do you know there will be any ? i said look , the english language only has a million words we 're hashing a million words into a table that can hold four billion the load factor on the table is negligible if we 're getting collisions it means we need a better hash algorithm there are plenty to choose from what we ought to do is just run the experiment and see if we even get any hash collisions assume we do get some how will you handle them ? well , i said , you can handle collisions via linked lists , or resize and rehash the table or just use a cuckoo hash algorithm and be done with it this led to a whole discussion of the cuckoo hashing algorithm which , amazingly , my inquisitor supposedly skilled in the art had never heard of this went on and on for quite a while we eventually discussed how to harvest the frequencies and create the desired sorted list but in the end , i returned to my main point , which was that sample noise and sample error are inevitably going to moot the results each time you run the program you 're going to get a different result if you do a fresh web crawl each time word frequencies are imprecise the lower the frequency , the more noise run the program on september , and you might find that the word terrorist ranks no in frequency on the web run it again on september , and you might find it ranks no that 's an extreme example vocabulary noise is pervasive , though , and at the level of words that rank no say on the frequency list , the day to day variance in word rank for any given word is going to be substantial it 's not even meaningful to talk about precision in the face of that much noise anyway , whether you agree with my analysis or not , you can see that a question like this can lead to a great deal of discussion in the course of a job interview , cutting across a potentially large number of subject domains it 's a question that leads naturally to more questions and that 's the best kind of question to ask in an interview