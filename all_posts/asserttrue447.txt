in science today , there 's a huge problem way too many people are publishing way too many results that are way too positive this problem is n't imaginary or speculative it 's rampant and well documented literature surveys have repeatedly uncovered a statistically unlikely preponderance of statistically significant positive findings in medicine see reference below , biology , ecology , psychology , economics , and sociology positive findings findings that support a hypothesis are more likely to be submitted for publication , and more likely to be accepted for publication they 're also published quicker than negative ones in addition , only of results presented initially in abstracts are eventually within years published , and those that are published tend to be positive there 's some evidence , too , that higher prestige journals or at least ones that get cited more often publish research reporting greater effect sizes why is this a problem ? when only positive results are reported , knowledge is being hidden a false picture of reality is presented e g , a false picture of the effectiveness of particular drugs or treatment options lack of informed decision making in health care is a serious issue , since lives are at stake but even when lives are not at stake , theories go awry when a hypothesis is verified in one experiment yet refuted in another experiment that goes unpublished because nothing was found another reason under reporting of negative findings is a pressing problem is that meta analyses and systematic literature reviews , which are an increasingly common and important tool for understanding the big problems in medicine , psychology , sociology , etc , suffer from a classic gigo conundrum the meta analyses are only as good as the available input if the input is fundamentally flawed in some way , odds are good that the meta analysis will also be flawed in some cases , scientists or drug companies simply fail to submit their own negative findings for publication but research journals are also to blame for not accepting papers that report negative results cornell university 's daryl bem famously raised eyebrows in when he published a paper in the journal of personality and social psychology showing that precognition exists and can be measured in the laboratory of course , in reality , precognition at least of the type tested by bem does n't exist , and the various teams of researchers who tried to replicate bem 's results were unable to do so when one of those teams submitted a paper to the journal of personality and social psychology , it was flatly rejected we do n't publish replication studies , the team was told the researchers in question then submitted their work to science brevia , where it also got turned down then they tried psychological science again , rejection eventually , a different team galak et al succeeded in getting its bem replication results which were negative published in the journal of personality and social psychology , but only after the journal came under huge criticism from the research community find more on this fracas here and here and here aside from the dual problems of researchers not submitting negative findings for publication and journals routinely rejecting such submissions , there 's a third aspect to the problem , which can be called outcome reporting bias putting the best possible spin on things this takes various forms , from changing the chosen outcomes measure after all the data are in to make the data look better , via a different criterion of success one of many criticisms of the million star d study of depression treatments , cherry picking trials or data points which should probably be called pea picking in honor of gregor mendel , who pioneered the technique , or the more insidious phenomenon of harking , hypothesizing after the results are known , all of which often occur with selective citation of concordant studies no one solution will suffice to fix this mess what we do know is that the journals , the professional associations , government agencies like fda , and scientists themselves have have tried in various ways and failed in various ways to address the issue and so , it 's still an issue a deterioration of ethical standards in science is the last thing any of us needs right now personally , i do n't want to see the government step in with laws and penalties instead , i 'd rather see professional bodies work together on a set of governing ethical concepts a explicit , detailed statement of ethical principles that everyone in science can voluntarily pledge to abide by journals , drug companies , and researchers need to be able to point to a set of explicit guidelines and be able to go on record as supporting those specific guidelines journals should make public the procedures by which disputes involving the publishability of results , or the adequacy of published results , can be aired and resolved in the long run , i think open access online journals will completely redefine academic publishing , and somewhere along the way , such journals will evolve a system of transparency that will be the death of dishonest or half honest research the old fashioned elsevier model will simply fade away , and with it , most of its problems over the short run , though , we have a legitimate emergency in the area of pharma research drug companies have compromised public safety with their well documented , time honored , ongoing practice of withholding unfavorable results the ultimate answer is something like what 's described at alltrials net it 's that , or class action hell forever references kyzas pa , denaxa kyza d , ioannidis jpa almost all articles on cancer prognostic markers report statistically significant results european journal of cancer here csada rd , james pc , espie rhm the file drawer problem of non significant results does it apply to biological research ? oikos here jennions md , moller ap publication bias in ecology and evolution an empirical assessment using the trim and fill method biological reviews here sterling td , rosenbaum wl , weinkam jj publication decisions revisited the effect of the outcome of statistical tests on the decision to publish and vice versa american statistician here mookerjee r a meta analysis of the export growth hypothesis economics letters here gerber as , malhotra n publication bias in empirical sociological research do arbitrary significance levels distort published results ? sociological methods research here song f , eastwood aj , gilbody s , duley l , sutton aj publication and related biases health technology assessment here dwan k , altman dg , arnaiz ja , bloom j , chan a w , et al systematic review of the empirical evidence of study publication bias and outcome reporting bias plos one e here hopewell s , clarke m , stewart l , tierney j time to publication for results of clinical trials review cochrane database of systematic reviews here scherer rw , langenberg p , von elm e full publication of results initially presented in abstracts cochrane database of systematic reviews here murtaugh pa journal quality , effect size , and publication bias in meta analysis ecology here kerr , norbert l harking hypothesizing after the results are known , personality and social psychology review , here etter jf , stapleton j citations to trials of nicotine replacement therapy were biased toward positive results and high impact factor journals journal of clinical epidemiology here