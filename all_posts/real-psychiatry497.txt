i was a fan of big data before it became fashionable i was a high tech investor before the dot com bubble and became very interested in high speed networking , especially the hardware necessary to move that data around even before that information was publicly available , electrical engineers were using that equipment to rapidly download large amounts of data gb from satellites on every orbit as an investor , one of the early flagship applications was large telescopes i wrote an article on high speed networks and the medical applications digital radiology and medical records back in at about the same time i made the information connection as a college student , i got my hands on the whole earth catalog that led me to my small college library and my surprise to find that they had shannon 's seminal work on information theory on the shelf i was even more excited when i learned about entropy in my physical chemistry course three years later since then i have been searching without much success to look at what happens when two people are sitting in a room and talking with one another my entire career has been spent talking with people for about an hour and generating a document about what happened it turns out that the document is stilted in the direction of tradition and government and insurance company requirements it covers a number of points that are historical and others that are observational the data is basically generated to match a pattern in my head that would allow for the generation of a diagnosis and a treatment plan the urgency of the situation can make the treatment plan into the priority the people who i am conversing with have various levels of enthusiasm for the interaction in some cases , they clearly believe that providing me with any useful data is not in their best interest others provide an excessive amount of detail and as the hour ends i often find myself scrambling to get to critical elements before the hour expires my current initial interview form has about categories this basic clinical interview in psychiatry has been the way that psychiatrists collect information for well over a century in the rest of medicine , the history and physical examination has become less important due to advances in technology as an example , it is rare to see a cardiologist these days who depends very much on a detailed physical examination when they know they are going to order an echocardiogram and get data from a more accurate source in psychiatry , other than information from a collateral interview and old records there is no more accurate source of information than the patient this creates problems when the patient has problems with recall , motivation , or other brain functions that get in the way of describing their history , subjective state , or impact on their life the central question about how much useful information has been communicated in the session , the signal to noise considerations , and what might be missing has never been determined the minimal threshold for data collection has never been determined in fact , every information specialist i have ever contacted has no idea how these variables might be determined information estimates have become more available over the past decade ranging from estimates of the total words spoken by humans in history to the total amount of all data produced in a given year estimates of total words ever spoken range from exabytes to zettabytes depending on whether the information is stored as typewritten words on paper or bit audio that , fold difference illustrates one of the technical problems what format is relevant and what data needs to be recorded in that format ? the spoken word whether recorded or typed is one channel but what about prosody and paralinguistic communication ? how can all of that be recorded and decoded ? is there enough machine intelligence out there to recognize the relevant patterns ? an article in this week 's nature illustrates the relative scope of the problem chris mattmann makes a compelling argument for both interdisciplinary cooperation and training a new generation of scientists who know enough computer science to analyze large data sets he gives the following examples of the size of these data sets one tb , gb project size encyclopedia of dna elements encode , tb us national climate assessment nasa projects , , tb fifth assessment report by the intergovernmental panel on climate change ipcc , due , tb square kilometer array ska , first light due , , , tb per year that means that the ska is nearly producing the total amount of information spoken by humans recorded as bit audio in recorded history every year the author points out that the ska will produce tb of data per second and within a few days will eclipse the current size of the internet ! all of this makes the characterization of human communication even more urgent we know that the human brain is an incredibly robust and efficient processor it allows us to communicate in unique and efficient ways even though psychiatrists focus on a small area of human behavior during a clinical interview the time is long past due to figure out what kind of communication is occurring there and how to improve it it is a potential source of big data and big data to correlate with the big data that is routinely generated by the human brain george dawson , md , dfapa dawson g high speed networks in medicine minnesota physician lyman , peter , h varian , k swearingen , p charles , n good , l jordan , j pal how much information ? berkeley school of information management systems mattmann ca computing a vision for data science nature jan doi a shannon ce a mathematical theory of communication the bell system technical journal