funnel graph for a meta analysis of studies involving cognitive behavioral therapy i was shocked yet greatly intrigued to encounter a paper on plos called why most published research findings are false , by john p a ioannidis , professor and chairman at the department of hygiene and epidemiology , university of ioannina school of medicine it appears both john and the university are named after the city of ioannina , greece far from a kook or an outsider , ioannidis is regarded as one of the world s foremost experts on the credibility of medical research using a variety of accepted techniques , he and his team have shown that much of what biomedical researchers publish as factual scientific evidence is almost certainly misleading , exaggerated , and or flat out wrong ioannidis alleges that as much as percent of the medical literature that doctors rely on is seriously flawed his work has been published in top journals where it is heavily cited and he is reportedly a big draw at conferences ioannidis 's efforts were given very serious consideration in a atlantic article called lies , damned lies , and medical science it turns out quite a bit of work has been done on developing analytical methods for inferring publication bias by a variety of statistical methods some of the techniques are summarized here for example , there are now such accepted methodologies as begg and mazumdar s rank correlation test , egger s regression , orwin s method , rosenthal 's file drawer , and the now widely used trim and fill method of duval and tweedie amazingly , at least four major software packages are available to aid detection of publication bias , for researchers doing meta analyses read about it all here some kind soul has put on scribd a complete copy of the book methods of meta analysis correcting error and bias in research findings , by john e hunter and frank l schmidt because the book is copyrighted , i do n't expect the scribd url to stay valid for long the book will probably be taken down at some point , but hopefully not before you 've had a chance to go there there are many factors to consider when looking for publication bias consider trial size people who do meta analysis of scientific literature have wanted , for some time , to have some reasonable way of compensating for the trial size of studies , because if you give small studies which often have large variances in results the same consideration as larger , more statistically significant studies , a handful of small studies with large effects sizes can swing the overall average effect size for all studies unduly this can be extremely worrisome in cases where small studies showing a negative result are simply withheld from publication and it does seem that the studies most likely to be kept unpublished are those that show negative results if you do a meta analysis of a large enough number of studies and plot the effect size on the x axis and standard error on the y axis giving rise to a funnel graph see the above illustration , you expect to find a more or less symmetrical distribution of results around some average effect size , or failing that , at least a roughly equal number of data points on each side of the mean for large studies , the standard error will tend to be small and data points will be high on the graph because standard error , as usually plotted , goes from high values at the bottom of the y axis to low numbers at the top see illustration above for small studies , the standard error tends of course to be large what meta analysis experts have found is that quite often , the higher the standard error which is to say , the smaller the study , the more likely the study in question is to report a strongly positive result so instead of a funnel graph with roughly equal data points on each side which is what you expect statistically , you get a graph that 's visibly lopsided to the right , indicating that publication bias from non publication of bad results is likely otherwise how do you account for the points mysteriously missing from the left side of the graph , in a graph that should by statistical odds have roughly equal numbers of points on both sides ? small studies are n't always the culprits some meta analyses , in some research fields , show funnel graph asymmetry at the top of the funnel as well as the bottom in other words , across all study sizes data points are missing on the left side of the funnel which is hard to account for in a statistical distribution that should show points on both sides , in roughly equal amounts the only realistic possibility is publication bias so is dr ioannidis right ? are most published research findings false ? i do n't think we have to go that far i think it 's reasonable to say that most papers are probably showing real data , obtained legitimately but we also have to admit there is a substantial phantom literature of unpublished data out there and we should definitely be concerned by that suppression of data can cost lives so this is far from a trivial matter i 've seen publication bias with my own eyes , in graduate school a postdoc in my lab at u c davis was trying to get a paper published in the prestigious journal of biological chemistry the paper this guy wanted to write revolved around a certain graph i watched this person repeat his experiment numerous times in order to get a data set that exactly matched the results he wanted to show on the graph he must have done the experiment nine times , and only once did it come out perfect and that 's what he submitted to jbc that 's eight suppressed data sets one published data set i saw enough of this as a grad student to turn me off to science altogether i quit the program about six months after being advanced to candidacy for a ph d gave up a regents fellowship , walked away from nih money i was young and impetuous real world science is bullshit , i said to myself it 's fraudulent , front to back i never thought anyone would blow the whistle on how science really operates i'm gratified to see that people like john ioannidis are now busy doing just that