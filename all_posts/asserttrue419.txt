claude shannon , the father of information theory , used to play an interesting game at cocktail parties he 'd grab a book , open it to a random page , and cover up all but the first letter on the page , then ask someone to guess the next letter if the person could n't guess , he 'd uncover the letter , then ask the person to guess the next letter suppose the first two letters are 'th' a reasonable guess for the next letter might be 'e' shannon would continue in this manner , keeping score , until a good deal of text had been guessed the further along one goes in this game , the easier it becomes of course to guess downstream letters , because the upstream letters provide valuable context what shannon consistently found from experiments of this sort is that well over half of english letters are redundant , because they can be guessed in advance in fact , shannon found that when all forms of redundancy are taken into account , english is more than redundant , with the average information content of a letter being approximately one bit per symbol yes , one bit see shannon 's prediction and entropy of printed english claude shannon shannon became intrigued by questions involving the efficiency of information transfer what is the nature of redundancy in an information stream ? are some encodings more redundant than others ? how can you quantify the redundancy ? eventually , shannon elaborated a mathematical theory around the encoding and decoding of messages that theory has since become extremely important for understanding questions of encryption , compression , detection of faint signals in the presence of noise , recovery of damaged signals , and so on a central concept in shannon 's theory is that of entropy shannon entropy is very widely misunderstood and or misinterpreted , so it 's important to be clear on what it 's not it 's not disorder entropy , in information theory , is not the same as entropy in thermodynamics , even though the mathematics are similar shannon liked to consider entropy a statistical parameter reflecting the amount of information or resolved uncertainty encoded , on average , by a symbol we think of the english alphabet as having symbols since values can be encoded in log bits , we say that the channel bandwidth for letter english is bits per symbol , but this is not the entropy shannon found that the entropy the actual bits used per symbol was closer to than to how can this be ? the answer has to do with the fact that some symbols are used far more often than others and also as noted , some symbols are redundant by virtue of context entropy gets to the actual rather than ideal information content of a message by taking into account actual frequencies of usage of symbols if english text used all letters of the alphabet equally and unpredictably , then the entropy of text would be exactly bits per symbol each symbol would contribute th of log to the total but because some letters are used more or less frequently than others , they contribute more or less than th of log , and that total can add up to less than it 's easy to visualize this with a simple example involving coin tossing suppose , for sake of example , that a series of coin tosses comprises a message as a medium of communication , the coin toss is capable of expressing only two states heads , or tails this could be represented in binary form as and if half of all tosses are heads and half are tails , then the total entropy in the message is log for heads plus log for tails , or one bit per symbol note if you actually do the math you 'll come up with a negative hence , in entropy calculations , the result is usually multiplied by so it can be expressed as a positive number consider now the situation of a two headed coin in this case , there is no tails term and the heads term is log , or zero this means the tossing of a two headed coin resolves no uncertainty and carries no information continuing the example , consider the case of a weighted penny that falls heads up two thirds of the time intuitively , we know that this kind of coin toss ca n't possibly convey as much information as a fair coin toss and indeed , if we calculate log for heads plus log for tails , we get an entropy value of bits per symbol , which means that each toss is on average or redundant if one were to take a large number of coin tosses involving the weighted penny and convert those tosses into symbols 'h' for heads and 't' for tails , say , the resulting data stream would be compressible to of its fully expanded size , and then it would n't compress any more beyond that , because that 's the entropy limit actually , that last statement needs to be qualified we 're assuming , throughout this example , that the result of any given coin toss does not depend on the outcome of the preceding toss if that rule is violated , then the true entropy of the message could be much lower than bits per symbol for example , suppose the result of successive coin tosses was h h t h h t h h t h h t there 's a recurring pattern , and the pattern makes the stream predictable predictability reduces entropy remember shannon 's cocktail party experiment you might ask yourself what a message with all possible redundancy removed would look like , and in what way or ways , if any , it would differ from apparent randomness technically speaking , when symbols represent independent choices not depending on what came before , the entropy can be calculated as before , and it 's called the order zero entropy but if any given symbol depends on the value of the immediately preceding symbol , we have to distinguish between order zero and order one entropy there are also order two , order three , and higher order entropies , representing contexts of contexts suppose now i tell you that an organism 's dna can contain only two types of base pairs gc and at you should be thinking coin toss suppose , further , i tell you that a particular organism 's dna is gc disregarding higher order entropy , does the dna contain redundancy ? if so , how much ? answer log for gc plus log for at equals , meaning redundancy is about could the actual redundancy be higher ? yes it depends what kinds of recurring patterns exist in the actual sequence of a , g , c , and t values there might be recurring motifs of many kinds each would send entropy lower further reading shannon 's best known paper , a mathematical theory of communication , bell systems tech journal , october a symbolical analysis of relay and switching circuits , shannon 's unpublished master 's thesis claude shannon 's contribution to computer chess shannon fano coding nyquist shannon sampling theorem