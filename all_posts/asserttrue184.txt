as a student of all things weird and wonderful , i have this horrible habit of stopping what i'm doing , in the middle of the work day , to investigate this or that oddball idea making a detour to codeville , and then five minutes later , an hour 's gone by but i 've learned a heck of a lot that 's kind of what happened yesterday , when i suddenly had to know not just how many words are in my latest book i already knew that , but what the total vocabulary of the book is how many unique words openoffice does n't tell you that thus began what should have been a five minute coding adventure but if you 're a coder , you know i'm talking code talk here , because five minute coding adventure actually means hour long bumbling misadventure leading to multiple head slap aha ! moments , a few of which i just have to share with you in terms of graphs prepare to be amazed fast forward to the graphs , if you do n't speak code getting the vocab count on my book of two minds was n't that hard first , i had openoffice output the book as html , then i opened it in firefox and in a scratchpad window typed the following code var text document body textcontent var wordmatch b w ' ? w b gi magic var thewords text match wordmatch var vocab new object get counts here for var i i ! totalmatches i if thewords i tolowercase in vocab vocab thewords i tolowercase bump count else vocab thewords i tolowercase start counting var results collect results here for var k in vocab results push k , vocab k the magic is in the regular expression b w ' ? w b gi , which is geek for match a word boundary , b , followed by one or more letters w , optionally followed by an apostrophe or hyphen , followed by zero or more word characters w , followed by a word boundary b , and do this globally in case insensitive manner gi thus , this regex will match ca n't as well as cant , but would n't 've correctly matched would n't 've , nor overly hyphenated expressions treating each of those as two words close enough , though , right ? the other lines of code show how to tally unique words in an object , which in js is just a hash table a table that will essentially let you use 'anyarbitrarytext' as a key to store and look up values once i had the results all unique words stored in a results array , i decided why not sort the array by word frequency ? bear in mind each array member is , itself , a mini array of word , count function comparatora , b var count a var count b return count count return results sort comparator all of this gets wrapped in a function , returning the sorted results array okay , so now the fun begins , because not only can i look at results length to get the vocabulary count which turns out to be huge over , but i can now make a graph of word count by rank order first i should tell you that around , junk words showed up in my initial attempt at a vocab count , due to things like single letter matches of people 's initials in footnotes r l smith gives three words , and other mischief , so i later modified the magic regex inserting a w to force recognition of just two letters or longer words which gave a vocab count of , the top words , by the way minus one letter words are word count the of to and in that it you for is was with on or at as be but not my geeks already know what 's coming next when you make a graph of word counts versus rank index , you find a curious phenomenon harvard linguistics professor george kingsley zipf was first to become fixated on the weird fact that the most frequent word , in english or any other language , will occur approximately twice as often as the second most frequent word , three times as often as the third most frequent word , and so on , implying a power law , which means skipping some math here that if you plot the logarithm of a word 's rank against the log of its frequency count , you should get a straight line here 's the plot for of two minds zipf plot of word frequency count vs rank order for words in of two minds click to enlarge as you can see , to a first approximation , the top words in of two minds show a striking power law distribution pattern classic zipf law i could have chosen the middle , words and gotten the same graph one of the characteristics of power law relationships is that they are scale invariant any part of the curve looks like any part of the curve but here 's the interesting thing since the code was already written and trivial to modify , i decided to extract the vocabulary of letter words and see if zipf 's law applied to just those words the graph i got looked like this zipf plot of letter words in of two minds the zipf relationship still holds arguably three letter words obey zipf 's law of course , being a degenerate code whore experimenter , i decided to go a step further and investigate letter words surely letter words are too perverse and unique in their special context requirements to obey any power law ? this is the graph zipf plot of letter words in of two minds a bit wavy , but still arguably a zipf law example at this point , drunk with power , i decided to wield my world destroying code wrangling abilities in a new way i looked at the stats for my top tweets you can download a spreadsheet of your twitter statistics , from twitter , sorted the impressions numbers to rank them , then did a plot of log impressions versus log rank , and guess what ? my top tweets of all time , plotted as log impressions vs log rank a classic zipf law situation again ! if anything , the relationship of tweet traffic to rank order is more zipf like than the relationship of word frequency to word rank so at this point , i'm thinking okay , i know that if i do the same trick using blog traffic , i'm apt to see a power law relationship again but i 've also long suspected that a couple of high traffic posts with way higher than normal traffic are acting as honeypots for bots and spiders can zipf 's law be used forensically , to detect out of whack numbers in a set of traffic stats ? i did a plot of log traffic vs log rank for the last posts on this blog log traffic vs log rank for posts on this blog notice the bulge caused by the rd and th points from the left on this plot those two blog posts , science on the desktop and nitric oxide , antidepressants , and sexual side effects , with combined traffic of over , page views , have always struck me as being bot bait , because as worthy as those two posts are , i ca n't see how they 'd draw that much human traffic , really , and the traffic numbers just keep piling on , week after week are people really coming back to those posts over and over again ? i think not i think our friend g k zipf is telling us that the two points that do n't fit the left part of this curve are spurious in other words , a zipf plot has forensic utility , because it can tell you which points do not obey the power law and are therefore errant in some fashion zipf law , a k a power law pareto , distributions are worth taking time to understand , because they underlie so many natural phenomena zipf pareto distributions accurately describe the size of craters on the moon , word frequencies in languages , sales curves for bestselling books , intensity of solar flares , citations of scientific papers , wealth distribution , city size distribution , and many other natural distributions see this excellent paper for details this is a fundamental phenomenon of nature , with huge forensic implications , because think about it , if zipf pareto describes bestselling book stats , you should be able to detect whether an author is gaming the system by looking for books whose sales stand out to an incredible degree in any system that follows zipf pareto laws , outliers points that do n't fall on the curve are suspect they have forensic importance , potentially are n't you glad you took this detour to codeville with me ? you can join the main highway again now coffee break 's over back to standing on our heads have you joined the mailing list ? what are you waiting for ? i want to thank the following great tweeps for retweeting me yesterday click into these profile pics and follow these people on twitter ! they retweet !