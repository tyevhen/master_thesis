Psychology research has a bullshit-results problem.

An effort to replicate 100 research findings in psychology turned up the disturbing fact that key findings from only 39 of the published studies could be reproduced.


From the 30 April 2015 article in Nature:
The results should convince everyone that psychology has a replicability problem, says Hal Pashler, a cognitive psychologist at the University of California, San Diego, and an author of one of the papers whose findings were successfully repeated.  “A lot of working scientists assume that if it’s published, it’s right,” he says. “This makes it hard to dismiss that there are still a lot of false positives in the literature.”

But Daniele Fanelli, who studies bias and scientific misconduct at Stanford University in California, says the results suggest that the reproducibility of findings in psychology does not necessarily lag behind that in other sciences. There is plenty of room for improvement, he adds, but earlier studies have suggested that reproducibility rates in cancer biology and drug discovery could be even lower. “From my expectations, these are not bad at all,” Fanelli says. “Though I have spoken to psychologists who are quite disappointed.”
In other words, these results aren't that bad, really, because they're just as shitty as in other, "harder" sciences!

The new project, known as the “Reproducibility Project: Psychology”, is the latest (and largest) of a wave of recent attempts to replicate previously published work, spurred by reports of fraud and fudged statistical analyses as well as heated arguments about whether classic psychology studies were appropriately robust. One such effort, the ‘Many Labs’ project, successfully reproduced the findings of 10 of 13 well-known studies. This was considered a success, of course. But it still suggests over a quarter of peer-reviewed science is bullshit non-replicable.

These results are not unexpected. The pressure on scientists and academics to publish "noteworthy results" is intense, and many studies routinely suffer from sponsor bias, researcher allegiance, a posteriori choice of outcome measures, lack of blinding, recruitment bias, misleading statistical techniques, and other well-known sources of bias. These sources of bias are rampant in psychological research (but also affect "mainstream science," of course). I talk about these problems in my books Mental Health Myths Debunked and Of Two Minds, which are not anti-psychiatry books per se but do focus on scientific research around depression, schizophrenia, suicide, meds, etc.—research that must be carefully scrutinized, if it is to be believed.

You can find many free sample chapters for Of Two Minds here, and much more on HackYourDepression.com.



 ☙ ❧

Tired of lies and half-truths about mental illness? Arm yourself with some facts. Check out my free book Mental Health Myths Debunked. Tons of info, tons of live links, lots of straight talk about depression, suicide, meds, therapy, psychiatry, mental health trends, statistics, and more. And you know me, I call bullshit on bogus ideas (then give URLs to the actual data, so you can weigh the evidence for yourself). The idea that antidepressants take weeks to do anything? Myth. Most people benefit from antidepressants? Myth. Antidepressants separate from placebo in clinical trials? Largely myth. (Half the trials show separation. Half don't.) Electroshock therapy is safe and effective? Bigtime myth. ECT is dangerous and consent forms are based on obsolete data. But don't take my word for it: Read the science for yourself. It's all laid out (with references) in the book. Download ePub or PDF now at NoiseTrade. Tell a friend.

☙ ❧ 
  I want to thank the following great folks who retweeted me yesterday on Twitter. My advice? Follow these guys. They retweet!



Have you added your name to our mailing list?

Also please visit HackYourDepression.com when you have a chance, and share that link with someone you know who might be suffering from anxiety or depression.