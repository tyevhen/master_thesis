Note: Some articles I mention below are without an abstract, so they received no link. The latest drama on mifepristone (RU-486, Corcept Therapeutics) continues to play out. DeBattista and Belanoff published a review (Trends in Endocrinology and Metabolism) discussing the great potential (in their eyes) for mifepristone in treating depression and possibly other conditions. Of course, let’s keep in mind that DeBattista owns shares in Corcept and is a consultant for them as well while Belanoff is the CEO of Corcept, so of course the review was park marketing, part science.

Robert Rubin and Bernard Carroll in a letter to the editor pointed out aptly that DeBattista and Belanoff’s review “makes exaggerated claims of efficacy.” They added that among the studies cited as proving evidence for mifepristone’s efficacy:
*One did not report a statistically significant benefit on any outcome measure
*Another “contained no statistical analyses of drug benefit whatsoever. The weak trend toward efficacy was created by alteration of the scoring method of a key rating scale (the Brief Psychiatric Rating Scale or BPRS). Had this scale been scored in accordance with its original psychometric validation, the results would have been even weaker than they were portrayed: only five, rather than 12, of 19 subjects who received high-dosage mifepristone would have shown a 50% reduction in the Positive Symptom Subscale (PSS) of the BPRS.
*The third study claimed a marginal benefit of mifepristone over placebo in producing a 50% reduction of scores on the PSS (p = 0.046). However, the statistical analysis failed to use Yates’ correction in computing the chi-square result. When the proper statistical test is used, or when the alternative Fisher exact test is used, then the result is clearly nonsignificant (p = 0.10). In addition, by analysis of variance the authors failed to find a significant main effect of mifepristone on any outcome measure of psychotic symptoms (with respect to the PSS) or general psychopathology (with respect to the BPRS) or depressive symptoms. The other two cited studies have appeared only in abstract form.

DeBattista and Belanoff, in their rather weak response, point out that one of the studies published only in abstract form is now in press and that it found significant results on the BPRS (as measured by a 30% reduction in scores) and several secondary measures. More on that study in a minute. They then point out that two small studies examining mifepristone which were criticized for failing to achieve statistically significant results by Carroll and Rubin yielded large effect sizes. I assume they mean pre-post effect sizes, meaning effect sizes that were not compared to placebo, as one of the studies did not use a placebo control. I’m not generally impressed when two small studies find large effects (not in comparison to a placebo) which do not carry over to larger studies. Note that DeBattista and Belanoff did not refute the points made about manipulating a rating scale to find a significant result or failing to use the proper correction for a statistical test, which also then pushed nonsignificant results to becoming significant. However, DeBattista and Belanoff make an excellent point when they state that “The first step with mifepristone is to demonstrate with greater confidence that the drug has a clinical effect.” Great point! That particular sentence sounds like a retreat from their earlier statements about mifepristone’s efficacy.

Now, as for that larger study on mifepristone for psychotic depression, which is now in press in Biological Psychiatry. DeBattista and Belanoff are the first two of eight total authors. The standard used to define response was a 30% reduction in BPRS scores. With a sample of 105 taking mifepristone and 116 taking placebo, the categorical analysis using Cochran-Mantel-Haenszel tests was barely significant (p = .041). Means and standard deviations were not reported for the BPRS, so who knows what happened in terms of effect size, but I’d bet it was not impressive. The results were more impressive for the Positive Symptom Subscale of the BPRS (p = .006), but it is again hard to calculate effect size without means and standard deviations.

The authors move on to reporting scores on a subgroup who had somewhat elevated Positive Symptom Scale Scores, and in this case means and SD’s are reported and it looks like about a .50 effect size favoring medication. How did this subgroup do on their overall BPRS score? Good question, because for this subgroup, only one measure (the aforementioned PSS) was reported. Apparently the patients with lower Positive Symptom Scores are not worthy of a subgroup analysis, likely because such an analysis would have shown unimpressive results.

The authors then move to another subgroup, 42 patients selected because they were “chronologically, the patients enrolled in the latter part of the study” and were being examined as part of an FDA analysis. This group showed the largest effects, according to my rudimentary analysis. It seems a little ridiculous to just report on the last 42 patients enrolled in the study. Why not report on the first 42? Oh, right, because when a group of 221 patients is enrolled, one draws a more valid conclusion from looking at the overall group rather than reporting on a random subset. Hello?? Maybe this is an attempt to send a message to the FDA – I’m not sure.

Now let’s move to the two biggest catches. One, there was no significant effect on depression (as measured by the Hamilton Depression Rating Scale). Let’s keep in mind that the patients had psychotic depression, which means treating the depressive symptoms might be nice.

The biggest catch was that everyone was allowed to take concomitant medication as their physician deemed appropriate. So the study really compared a mishmash of drugs plus mifepristone to a mishmash of drugs plus placebo. Not exactly the type of tightly controlled efficacy trial upon which one can make many conclusions. Especially considering that the authors reported little about which drugs were prescribed to which patients and how this may have influenced outcomes.

So, in the end, the study showed very little. More points to Rubin and Carroll. Methinks we need more people in psychiatry who are willing to throw the BS flag when they see products being overhyped.

I will say one good thing about the DeBattista et al study. The following quote was great: “The discovery of psychotropics to date has rested on serendipity and repetition of drugs with similar pharmacological profiles.” Unfortunately for them (as Corcept shareholders), the data do not support that their unique drug works any better than a me-too drug, and it is still unclear if mifepristone is much better than a sugar pill.

Link to an earlier post involving Carroll and Rubin here. Keep up the good work fellas.